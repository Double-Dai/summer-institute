# Lesson plan for instructors
## Summer Institute in Computational Social Science 2020
## Activity, Day 4: Non-probability-based Surveys in Practice
## Original plan Matthew Salganik & Robin Lee [1], modifications by Nick Camp, Jae Yeon Kim, and Jaren Haber 

### This lesson plan replaces data collection with MTurk by having participants use pre-existing sources of data: either SICSS Duke's MTurk Survey Results, or publicly available data.

### Instructor preparation

To prepare for this activity you should:
- Carefully read the [lesson plan for participants](lesson_plan_survey_participant.md) and the reading assignments included in it.

### Group formation

Form groups of about 3 people. We changed the recommendation of groups of 5 to 3 this year because we foresee more effective remote collaboration with smaller group.  If possible, each group should have one person with experience in surveys and one person with experience in MTurk.  The more groups you have, the higher the costs of data collection because each group will collect some data.

### Key Tasks for Organizers and TA if you do use MTurk for data collection

- Double-check groups have paid Turk workers by 2PM. When there is a doubt about whether the worker has completed the survey correctly, please just pay the worker.

### Potential Discussion Questions

When you come back for the group discussion, here are some possible questions:
- (If using estimates from Pew): How close were the estimates for this activity to the estimates from Pew?  What do you think might be causing some of these differences? How could you test this speculations?
- What issues have you noticed while you’re implementing survey in Google Form that might affect how respondents answered the questions?
- How important is it to open-source data that you have collected with other researchers?
- What are things we should keep in mind when open-sourcing data?
- Does this actvity change how you think about your own research going forward?

### More information about Pew Research Center methodology

We draw the questions from Pew Research Center. To minimize the need to do data wrangling, we only picked questions that yield binary responses. Below is a summary of the methodology. Read more about the methodology [here](https://www.pewresearch.org/methods/u-s-survey-research/american-trends-panel/).

#### Recruitment

The current respondents were recruited through both phone call survey and mail-in forms.

Pew Research created the American Trends Panel (ATP) in 2014. Between 2014 and 2017, the panelists are invited to join the panel at the end of a large, national, landline and cellphone random-digit-dial survey that was conducted in both English and Spanish.

In August 2018, the ATP switched from telephone to address-based recruitment. Invitations were sent to a random, address-based sample (ABS) of households selected from the U.S. Postal Service’s Delivery Sequence File. In each household, the adult with the next birthday was asked to go online to complete a survey, at the end of which they were invited to join the panel. Households without internet access were instructed to return a postcard. These households were contacted by telephone and sent a tablet and free internet service if they agreed to participate.

The U.S. Postal Service’s Delivery Sequence File has been estimated to cover as much as 98% of the population, although some studies suggest that the coverage could be in the low 90% range.

![Recruitment methods for the panel respondents](https://www.pewresearch.org/methods/wp-content/uploads/sites/10/2019/12/12.12.19_ATP-update.png?resize=640,510)

#### Administration of the survey

ATP is done by self-administration. It is not administered over phone interviewing. When a survey becomes available, panelists were notified via text messages, email or tablet notification depending on their consent options. Panelists can fill out the survey using smartphone, laptops/desktops or tablets.
