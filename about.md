---
title: 'About'
subtitle: 'A collection of post-mortems'
layout: about
---

# Summer Institutes in Computational Social Science 2020 Post-mortem

__Published on:__ August 19, 2020


The SICSS Festival 2020 will take place Monday, June 22 - Friday, June 26, 2020. During the festival, alumni from all SICSS locations would host events such as tutorials, panel discussions, or debates.  These events will all be online.  SICSS Festival events will be open to either current SICSS participants (at any location) or anyone that is interested; the choice of audience will be driven by the topic, learning objectives, and preferences of presenters.  If you are a SICSS alumni and would like to host an event in the festival, please send us an email with a rough sketch of your idea.  We will be adding events to this website as they are finalized.

- SICSS-Bay Area organized by Jae Yeon Kim (SICSS-Princeton 19), Jaren Haber (SICSS-Princeton 19), and Nick Camp (SICSS-Princeton 19).
- SICSS-Istanbul organized by Akın Ünver (SICSS-Kadir Has University 19) and Matti Nelimarkka (SICSS-Kadir Has University 19, SICSS-Helsinki 18, SICSS-Princeton 17)
- SICSS-Maastricht organized by Monika Leszczyńska (SICSS-Princeton 19) and Catalina Goanta
- SICSS-Montreal organized by Vissého Adjiwanou (SICSS-Cape Town 18, 19, SICSS-Princeton 17) and Julie Hussin
- SICSS-Rutgers organized by Katherine McCabe (SICSS-Princeton 19), Hana Shepherd, and Kira Sanbonmatsu
- SICSS-Stellenbosch organized by Aldu Cornelissen (SICSS Cape Town 18), Douglas Parry (SICSS Cape Town 19), and Richard Barnett (SICSS Cape Town 18)
- SICSS-UCLA organized by Alina Arseniev-Koehler (SICSS-University of Washington 18, SICSS-UCLA 19), Jennie E. Brand (SICSS-UCLA 19), Pablo Geraldo Bastías (SICSS-UCLA 19), and Bernard Koch (SICSS-UCLA 19)

In addition to the locations that happened virtually, we have 13 locations postponed because of COVID: SICSS-Beijing, SICSS-Chicago, SICSS-Copenhagen, SICSS-HSE University, SICSS-Helsinki, SICSS-Howard-Mathematica, SICSS-Konstanz, SICSS-Milano, SICSS-NYU, SICSS-Oxford, SICSS-Paris, SICSS-Princeton-CITP, and SICSS-Tokyo.   

The purpose of this blog post is to describe a) what we did, b) what we think worked well, and c) what we will do differently next time.  We hope that this document will be useful to other people organizing similar Summer Institutes, as well as people who are organizing partner locations for the 2021 Summer Institutes in Computational Social Science.  If you are interested in hosting a partner location of SICSS 2021 at your university, company, NGO, or governmental organization, please read our information for potential partner locations.

This post includes post-mortem reports from all of our locations in order to facilitate comparisons.  As you will see, different sites did things differently, and think that this kind of customization was an important part of how we were successful.


## SICSS-Duke organized by Chris Bail and Matthew Salganik


We’ve divided this post into 7 main sections: 1) outreach and application process; 2) pre-arrival and onboarding; 3) pre-recording of lectures; 4) first week; 5) second week (group projects); 6) second week (SICSS Festival); 7) post-departure.


### 1. Outreach and application process


We continue to think that the best way to have a great Summer Institute is to have great participants.  As in previous years, we advertised our event to  a large, diverse group  Our major outreach effort began in January— once almost all of the partner locations had been finalized. We emailed former participants and former speakers.  We also advertised through professional societies and asked our funders to help spread the word. Finally, we tried to reach potentially interested participants through social media, email lists, and emails to faculty that we thought might know interested participants.  We made a special effort to reach out to faculty that we thought might know people from groups that are under-represented in our applicant pool.  We were happy to learn that this year many participants heard about the Summer Institutes from a former participant.

Managing the application process was better this year than last year.  In 2019, at the request of our funder, the Russell Sage Foundation (RSF), we switched to Fluxx (partner locations are not required to use Fluxx and are not allowed to use the RSF instance of Fluxx).  Based on what we learned in 2019, we improved the process and it went pretty smoothly.   A key change was looping in staff at RSF when there were problems with Fluxx. It was particularly helpful for there to be a specific person at RSF for the TA on application support to message when applicants had issues with their submissions. One challenge that has come up in all three years is the difficulty of accepting letters of reference. This always proves one of the most logistically difficult parts, and we would urge locations to consider these costs when contemplating requesting letters.

We received all of our applications before COVID, and we did our selection of candidates before we knew that the event would be virtual.  After we decided that we would have SICSS-Duke and that it would be virtual, we informed applicants that they were accepted and the event would be virtual.  We were quite happy that our yield was close to 100%.  

When selecting participants, we did not consider what time zone they lived in.  But, time zones turned out to be a barrier for some participants.  All of our events were based on Eastern Time US (the timezone of Duke).  Most of our participants were located between Pacific Time US and Eastern Time US, and they could take part without much difficulty.  A few participants were located in Europe, and talks that happened in the evenings (Eastern Time US) were harder for them to attend. One participant was based in Australia, and this made it very difficult for him to fully participate.  One of the participants we admitted who lives in Asia decided not to participate because of the time change.


### 2. Pre-arrival and onboarding


After participants are accepted we begin to onboard them into the program and provide them with pre-arrival materials.  The goal is to have them arrive at SICSS ready to learn.

We added all participants and staff to a Google group and sent out an email to the group requesting participant bio information for our website and providing participants information on pre-arrival logistics, such as how to join the Slack workspace. To collect information from participants, we used a Google form that we linked to in the email.

On the Google form, we asked for:

- name (as it should be displayed on the website)
- personal website link, if desired
- short bio for the website
- photo to display on the website
- previous coding experience
- preferred contact email
- preferred T-shirt size

The use of one Google form minimizes the number of emails.

In addition to requesting this information, the email reminded participants to read the pre-arrival section of the website, gave them a link to use to join the SICSS 2020 Slack workspace, requested that they join the #sicss-duke and #pre-office-hours channels, gave some additional detail about videoconference plans and linked to an article providing tips for improving their experience with Zoom, and requested T-shirt designs. 

After sending email reminders to a handful of participants, eventually everyone completed the Google form. A few participants never submitted profile pictures.

Unfortunately, we forgot to ask for a T-shirt shipping address, so we sent out an additional form the week before the event to collect the shipping address and shirt size (again). This form went to participants at all SICSS locations. In future, if we are shipping shirts individually, we should ask for the shipping address in the original onboarding form, using a template that is friendly to international addresses (the defaults for this in Google Forms can be US-specific). The vendor used for US/Canada shipping was createmytee.com and for International was customink.com. 

T-shirts are a SICSS tradition, and we think they are a great way to build community.  Because the event was virtual this year, we shipped t-shirts to each individual participant who requested one. This ended up being a lot of logistical work and a fairly high financial cost. While t-shirt vendors are increasingly adding the capability of bulk ordering with individual shipping, there was not a large selection, and there was a flat shipping fee added for each individual. Additionally, very few vendors were able to ship individually internationally. To try to accommodate these issues while staying close to budget, we ordered t-shirts from two vendors – one for all participants living in the US and Canada, and another for participants who live outside of the US and Canada. The website for the US vendor (createmytee) said on their website that there was “always free shipping.” This was not actually true in the case of individual shipping. 

To avoid these issues in the future, we should aim to have an estimate of the number of people who want a shirt and their locations at least 2 weeks before the actual event so the logistics can be worked out prior to event responsibilities. To help with issues of international/national shipping, one possibility could be having a central t-shirt design and then having site organizers be in charge of placing orders for participants attending their site. 

Another component of the pre-arrival support is office hours run by our TAs.  These office hours are open to participants at all SICSS locations.  We provided 6 weeks of office hours from 5 TAs. We tried to spread out different times so that people from different time zones can attend at least one if needed. Few attended the sessions. But for the ones who attended, TAs were able to help them set up R and discuss potential project ideas. 


### 3. Pre-recording of lectures


One major change this year is that we switched to a “flipped classroom” model, which required us to pre-record our lectures.  We decided that the quality of these videos was important so we invested a lot of work into making them high-quality.  This was a huge amount of work, but participants reported that they turned out well.  The work mainly consisted of two parts.  First, the lectures needed to be modified to fit into smaller chunks.  All the advice we received was that they should be no longer than 30 minutes.  Second, and even more time consuming, was the need to get equipment and actually record them.  We each used a high-quality camera, three lights, a high-quality green screen, and a lav mic.  Getting this equipment during the early stages of the pandemic was challenging, and it was time consuming to set-up and learn how to use it correctly.  We also found it much harder lecturing to a camera than lecturing to a live room of participants.  We worked with a professional videographer, Corey Reid, and he helped us so much, including recommending equipment, helping us set it up, and then editing the final videos.  We are grateful to Corey and if you are looking to produce some videos like ours and you are looking for help, you should get in touch with Corey.

One challenge for Matt is that much of his lecture materials overlaps with his book Bit by Bit: Social Research in the Digital Age. After a bunch of deliberation, Matt decided to restructure his lectures so that the material from Bit by Bit was separated from the additions and extensions.  For the areas of overlaps, participants could watch the video, read the book, or both.  For the additions and extensions, they had to watch the videos.  Ultimately, we think this was the right choice, but we could have explained it more clearly to our participants.

__Week 1__

The first week of SICSS is traditionally a mix of lectures and group activities.  This year, to reduce Zoom fatigue, we pre-recorded our lectures and asked participants to watch them before arriving.  We also reduced the length of the day (e.g., starting at 10am rather than 9am), added more breaks, reduced the number of guest speakers, and made more events optional.  Overall, we think these changes were necessary and reasonably effective.

We began the first week with a virtual meet-and-greet on Sunday evening, largely following this model. We encouraged participants to read each other’s bios before the event. After a few words from Chris, participants and staff were randomized into groups of four for 30 minutes, then randomized into a different group of four for another 30 minutes. There was a final optional 30-minute interval. Feedback on this structure was quite positive. Groups of four seemed to be roughly the right size.

We used several different models during the 5 days of instruction, and we received different feedback on different days.  It is hard to know how much of the results were attributed to our instructional choices, as opposed to the content of the day, the fact that participants were getting more familiar with Zoom and each other, and the fact that participants were generally getting more exhausted.  All days followed a structure where participants were split into smaller groups to work together on activities (which are all available from our website).  What differed across days was how open-ended the activities were, how multi-dimensional the activities were (i.e., did they require a mix of skills or just a single skill), the sizes of the groups (between 3 and 5), how the groups were formed (e.g., random or designed to be mixed skills), and whether we came back together at the end of the day for participants to share and discuss what they did in their smaller groups.  It is not clear if there are right answers to any of these decisions, but we think that each should be made explicitly based on the learning objectives for that day.

As expected, collaboration over Zoom proved tricky, especially for activities that required coding collaboratively. Different groups experimented with different ways to navigate this, including screen sharing, working individually and passing files back and forth to one another, and controlling each others’ computers via Zoom remote control. Using Zoom remote control requires users to grant Zoom security and privacy access in accessibility (MacOS). This requires allowing others to control a user’s computer, which we viewed as a potential serious limitation. None of these solutions are ideal. It would be preferable to find a platform to facilitate real-time collaborative code editing. We think screen.so may hold promise in this regard. Similar to remote control via Zoom, screen.so requires privacy access but users can specify the window that other users can have control. Remote control does not address the issue of having incompatible computing environments. There were many other tools, but unfortunately we were unable to find another service that was both free and allowed for collaborative, real-time group editing. We also tested RStudio Cloud, which does not allow real-time group editing, and CoCalc, which only allows real-time group editing with a paid subscription. No participants opted to try screen.so or the other collaborative platforms. Most groups simply shared screens, but we received feedback from some participants that this format encouraged the most experienced coders to take over, and made it difficult for less experienced coders to participate. 

As with events in person, it was difficult in the virtual format to ensure a relatively equal balance of participation.  Sometimes a small number of people dominated the discussion and other people did not participate.  We received a few comments about this issue in the keep-start-stop surveys throughout week 1. It seemed to be an issue in both breakout rooms and large group settings. Smaller groups (e.g., 3 people) and more structured activities seemed to mitigate these problems somewhat. Another challenge to shared participation was the choice of programming language. Most participants prefer R and some Python. We did not choose to separate people based on their preferred languages. One reason is that collaboration in computational social science may involve people working together using tools they’re not familiar with. 

For many events, we used Zoom breakout rooms for small group work. Creating random breakout rooms in Zoom is quite easy, and making other instructors co-hosts let them move between rooms. However, if someone leaves the Zoom call and then comes back, Zoom does not remember what room they were in. Internet connectivity issues occasionally caused participants to leave and then come back, and the TA who was the host of each meeting could easily re-assign them to the correct room. However, after lunch breaks everyone would come back at once, which caused some delay in getting everyone back into their rooms because they all had to be re-assigned. When an instructor took some time at the start of the post-lunch session to set up the next part of the activity, this delay was less noticeable for the participants. Something we could have done pre-arrival to make this process easier was to collect the email address each participant uses for Zoom. This address is often a university-set field, so is sometimes different from the email they give us for regular communication. With preset breakout rooms using those emails, groups can be re-formed instantaneously to ease transition coming back from lunch. 

At an in-person SICSS a lot of learning and community building happens in lunches. To mimic the experience of sharing meals with fellow participants, we opened a few breakout rooms from 12-1 each day in Week 1. We selected a conversation topic for each room beforehand from suggestions participants gave in Slack. We made all participants co-hosts of the Zoom meeting so that they could move between rooms at will. One room was always devoted to non-academic topics. Many of the suggested topics were very generative. To prevent Zoom fatigue, we made attending the lunch conversations optional. Attendance was highest on the first day and dropped thereafter. On some days, participants coalesced on just one of the rooms, leaving in effect just one topic. We found that having TAs evenly distribute between rooms at the start of lunch mitigated this issue somewhat.

__Week 2 (Group Projects)__

A major part of SICSS is participant-led group research projects during the second week.  This year, because of the online nature of the event and the challenging nature of the times, we decided to make the group projects optional.

In our research group matching process, we used a google spreadsheet for people to add research interests. As people are adding their research interests live, we need a way of telling people to stop adding research interests, and to make sure they add zeros and ones to all of the cells. To do this, it would be more effective to have everyone in the same zoom at the beginning of this exercise.

It is much more difficult to keep track of group projects in a virtual setting. Though we had a list of project ideas and very tentative group assignments from early in the week, this changed substantially through the week in ways that were difficult to track. By the end of the week, we were no longer sure how many participants were participating in group projects, and we were much more aware of some projects than others. Some groups remained on the scheduled Zoom link to be in the breakout rooms. Some met separately. This made it difficult to check in with groups and provide them support. In future, we should ask participants to record which group they ended up joining and provide a short description of what that group is doing, perhaps on Tuesday, after this is mostly settled or perhaps at the end of each day. This way we can be sure to check in with all active groups on a more regular basis. 

Running the group projects through one central MTurk worked although it was somewhat difficult to coordinate times when everyone was available to set up and run the HIT. In the future, if one person is going to run several projects through a central MTurk account, a possibility could be making a Google Form for research groups to submit: the link to their survey, the name and description of their survey, the number of participants, the payment they want per participant, and any restrictions on participants. In this way, the person running the MTurk could have all that information on hand when running the HIT for the participants without having to find a time to screen-share on zoom or constantly messaging on Slack. 

One other question that came up several times was how IRB approval works with small research projects. Pre-empting the confusion around IRB, in the future we should state clearly the expectations at the end of research speed dating and on the second day is a good idea. 

Creating an MTurk/Prolific account and linked gmail account for each site and depositing a set amount of research funds that site organizers could use at their discretion worked well. Having them set up in the week prior to the event in the future would likely be helpful, just in easing communication and ensuring everyone can access the account prior to the event. 

Some groups at the Duke site were also interested in doing research not on MTurk. Looking into using Prolific more broadly could be one option or making it clear the types of funds we can accommodate versus not accommodate.

On Friday there were three groups presenting their group projects. While only half of the attendees were involved in one of the three projects, around twenty attendees attended the closing presentation and gave thoughtful feedback. 

__Week 2 (SICSS Festival)__

This year we launched the first-ever SICSS Festival.  During the Festival, alumni from all SICSS locations hosted events such as tutorials and panel discussions.  With the Festival we were hoping to provide learning opportunities to a larger and more diverse set of people than those who can commit to attending a two week long program.  We also wanted to provide an opportunity to showcase the contributions and expertise of our amazing alumni.

Initially, we decided to try to host about 5 events (one per day) in order to cover a range of topics and also not spread our audience too thin.  The SICSS-Duke organizers and Festival organizers worked together to brainstorm panel ideas that would feature alumni.  Then we emailed alumni that we knew were passionate about these topics asking them if they wanted to participate.  In all cases the alumni said yes, and sometimes they even suggested other alumni to include.  Once all panelists agreed, we created a shared google doc for each panel where we participants could comment on the proposed description, suggest questions, and leave notes.  Having one google doc that held all the information about each event was a good structure and preventing things from getting lost in email.  After we announced the Festival to the SICSS alumni, we received additional proposals for panels.  In the end we hosted about 10 events.

Many panels could be split into about 5 chunks.  In the initial chunk the moderator kicked off the event.  This kick-off often included: telling the audience about SICSS and the SICSS Festival, providing a rough schedule for the event, providing information about how and when the audience can ask questions (we used both a mix of chat, video, and Zoom’s Q&A feature depending on the event), a reminder that the event is being recorded (if applicable), and a bio of the speaker.  The second chunk of the panel involved the moderator and panelists in conversation.  The third chunk involved the transition to audience questions.  The forth chunk involved audience questions.  The fifth chunk involved wrapping up, including thanking everyone, reminding them that they will receive a feedback form that we will share with the speaker, and reminding them about upcoming events. In some panels, we created a 10-15 minutes informal conversation time during which we have stopped recording. This provided a space for some attendees to talk to panelists informally. 

Platform: We hosted our events on Zoom. This generally worked well, even for large events.  However, we did hear reports that Zoom is blocked in certain countries so these participants were only able to watch recordings of events.  In future years, the choice of video platforms should include consideration of at least the following dimensions: quality of audio and video; familiarity for presenters, audience, and organizers; cost; and accessibility.

Mix of events: Looking back, it is now clear that most—but not all—events were targeted to people already in the computational social science community, rather than folks hoping to join the community.  If a goal of the Festival is to provide on-ramps to computational social science, then the mix of events should be reconsidered.

Time commitment: The Festival consumed a lot of time for one organizer and one TA during the second week.  This limits the ability of these teaching staff to support group research projects.  One way to mitigate this would be to have fewer events moderated by the organizer.  Rather, they could be moderated by alumni or TAs.  In addition to organizer time, many SICSS participants reported that it was difficult to attend the events while working on their group projects.

Using chat and email:  As the panel was happening, the panelists would often mention papers or resources.  We would then put these links into the chat so that participants could access them.  We also collected up all the links shared during each event, and emailed them to registered participants afterwards.

Feedback: At the end of each event, we emailed a link to a feedback form to all participants.  Response rates were low and seemed to decline over the week. Most feedback was very positive.  The most common suggestions were to make the events longer or to add more time for questions from the audience.

Registration and attendance: Attendance at the events was quite good, and we had about 650 total attendees.  We also found that about ⅓ of registered participants actually attended.  

- Panel discussion on teaching computational social science, 240 registered, 80 attended
- Measuring cultural change in digital trace data using diversification rates, 150 registered, 60 attended
- Discussion on diversity in computational social science, 190 registered, 80 attended
- Computational social science to address the (post) COVID-19 reality, 325 registered, 80 attended
- Panel discussion on digital and computational demography, 140 registered, 60 attended
- Using Empirica for high-throughput virtual lab experiments (Session 1) 30 registered (capped), 25 attended
- Creating open source software as part of an academic career, 170 registered, 60 attended
- Panel discussion on the non-academic job market in computational social science, 250 registered, 130 attendees
- What Can the SICSS Community Do to Recognize and Eradicate Anti-Black Racism in Computational Social Science? 55 registered (restricted to SICSS current participants and alumni), 30 attendees
- Opportunities and challenges with industry collaborations 200 registered, 40 attendees
- Using Empirica for high-throughput virtual lab experiments (Session 2) 30 registered (capped), 20 attended


### Post-departure


This year it feels like there is less to do post-departure.  We have organized our teaching materials and the teaching materials from other locations.  We have also read through the evaluation form from participants at our location and other locations.  Finally, we have compiled this post-mortem, which we hope will help with next year’s SICSS.